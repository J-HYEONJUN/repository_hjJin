{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e7d119f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "import ssl\n",
    "from nltk import word_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03a0892e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\hj00m\\anaconda3\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\hj00m\\anaconda3\\lib\\site-packages (from openpyxl) (1.1.0)\n",
      "                               reviewId             userName  \\\n",
      "0  8b33921b-b971-49ad-9c43-5f46535d8481       Rhylenn Taylor   \n",
      "1  ddfa4628-ecd7-403d-b4f8-976655986017      Addalaide Clark   \n",
      "2  8d9d55dd-df25-4329-af58-2178007c5434  jennavie kajornklin   \n",
      "3  493e1a17-3af8-4e60-b5d2-a1f9f1056355              Josilyn   \n",
      "4  4ac6c4f5-c019-46f0-b829-8e98d19a93ea                  Eli   \n",
      "\n",
      "                                             content  score  thumbsUpCount  \\\n",
      "0  Perfect App! Though, There are a few problems ...      5            609   \n",
      "1  I love this game. I have so much fun playing w...      4           1384   \n",
      "2  Roblox is my favorite game. But when I play on...      4            576   \n",
      "3  This game is great, amazing almost. But my sit...      1            166   \n",
      "4  been glitching and completely freezing in the ...      1            143   \n",
      "\n",
      "  reviewCreatedVersion                  at appVersion  \n",
      "0            2.578.564 2023-06-24 03:07:01  2.578.564  \n",
      "1            2.578.564 2023-06-25 16:16:14  2.578.564  \n",
      "2            2.578.564 2023-06-21 12:12:21  2.578.564  \n",
      "3            2.578.564 2023-06-24 16:21:53  2.578.564  \n",
      "4            2.578.564 2023-06-24 02:22:29  2.578.564  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "!pip install openpyxl\n",
    "\n",
    "file_path = 'C:/Users/hj00m/OneDrive - 금오공과대학교/3. Lab/2. 김민준_교수님/2. data/ROBLOX_DATA_PS.xlsx'\n",
    "\n",
    "# 엑셀 파일 읽기\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# 데이터프레임 출력\n",
    "print(df.head())  # 처음 5개 행 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69bc1c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "중복된 행 수: 0\n",
      "중복된 행이 없습니다.\n"
     ]
    }
   ],
   "source": [
    "duplicates = df[df.duplicated(subset=['reviewId', 'at', 'content', 'score'], keep=False)]\n",
    "print(f\"중복된 행 수: {len(duplicates)}\")\n",
    "\n",
    "if not duplicates.empty:\n",
    "    df = df.drop_duplicates(subset=['reviewId', 'at', 'content', 'score'], keep='first')\n",
    "    print(\"중복된 행을 제거했습니다.\")\n",
    "else:\n",
    "    print(\"중복된 행이 없습니다.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6ab30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK 다운로드 설정\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "except AttributeError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84123937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\hj00m\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\hj00m\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\hj00m\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\hj00m\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# 다운로드\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# 전처리\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"\n",
    "        u\"\\U0001F300-\\U0001F5FF\"\n",
    "        u\"\\U0001F680-\\U0001F6FF\"\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return 'a'\n",
    "    elif tag.startswith('V'):\n",
    "        return 'v'\n",
    "    elif tag.startswith('N'):\n",
    "        return 'n'\n",
    "    elif tag.startswith('R'):\n",
    "        return 'r'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    text = text.lower()\n",
    "    text = remove_emoji(text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tagged_tokens = pos_tag(tokens)\n",
    "    lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_tokens]\n",
    "    return lemmatized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b9e0ec7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64ddfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰화\n",
    "df['tokens'] = df['content'].apply(preprocess_text)\n",
    "df['token_count'] = df['tokens'].apply(len)\n",
    "df_filtered = df[df['token_count'] >= 10].copy()\n",
    "\n",
    "print(df_filtered.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a87ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LDA\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "texts = df_filtered['tokens'].tolist()\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "lda_model = LdaModel(corpus=corpus,\n",
    "                     id2word=dictionary,\n",
    "                     num_topics=9,\n",
    "                     passes=10,\n",
    "                     random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a4da46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 keywords: ['account', 'get', 'roblox', 'say', 'log', 'try', 'back', 'cant', 'update', 'work', 'go', 'let', 'please', 'password', 'delete']\n",
      "Topic 2 keywords: ['robux', 'game', 'get', 'buy', 'like', 'money', 'free', 'dont', 'need', 'thing', 'make', 'wish', 'good', 'love', 'pay']\n",
      "Topic 3 keywords: ['game', 'like', 'people', 'say', 'dont', 'good', 'get', 'bad', 'thing', 'really', 'roblox', 'fun', 'hacker', 'word', 'ban']\n",
      "Topic 4 keywords: ['kid', 'great', 'game', 'roblox', 'app', 'good', 'would', 'child', 'think', 'age', 'amaze', 'recommend', 'way', 'make', 'use']\n",
      "Topic 5 keywords: ['na', 'add', 'wan', 'remove', 'gon', 'pls', 'back', 'please', 'plz', 'outfit', 'bring', 'skin', 'hair', 'premium', 'jailbreak']\n",
      "Topic 6 keywords: ['game', 'play', 'fun', 'love', 'friend', 'like', 'roblox', 'make', 'best', 'really', 'much', 'many', 'get', 'ever', 'app']\n",
      "Topic 7 keywords: ['game', 'fix', 'play', 'roblox', 'good', 'please', 'cant', 'lag', 'like', 'bug', 'problem', 'phone', 'sometimes', 'work', 'love']\n",
      "Topic 8 keywords: ['star', 'give', '5', 'game', 'update', '4', 'take', 'rate', 'five', 'good', 'would', 'love', 'like', '3', 'upgrade']\n",
      "Topic 9 keywords: ['u', 'year', 'play', 'ive', 'playing', 'love', 'ur', 'im', 'since', 'roblox', 'day', 'get', '3', 'old', 'best']\n"
     ]
    }
   ],
   "source": [
    "# 키워드 추출\n",
    "topic_keywords = []\n",
    "for i in range(9):\n",
    "    words = lda_model.show_topic(i, topn=15)\n",
    "    keywords = [word for word, _ in words]\n",
    "    topic_keywords.append(keywords)\n",
    "\n",
    "for idx, kw in enumerate(topic_keywords):\n",
    "    print(f\"Topic {idx+1} keywords:\", kw)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c45867",
   "metadata": {},
   "source": [
    "키워드 유사어 확장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2227628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1 확장 키워드 (47개): ['cant', 'accout', 'guyss', 'updates', 'account', 'butwhen', 'git', 'exited', 'workbut', 'get', 'barley', 'allow', 'backand', 'plz', 'pasword', 'deleted', 'updatei', 'woke', 'sign', 'deleting', 'please', 'updating', 'work', 'laging', 'kindof', 'login', 'happenedi', 'couldnt', 'fricken', 'worked', 'back', 'password', 'return', 'tryin', 'delete', 'passwordi', 'gat', 'pls', 'incorrect', 'log', 'game', 'deleate', 'go', 'logout', 'lem', 'roblox', 'init']\n",
      "Topic 2 확장 키워드 (47개): ['rproleplay', 'gamess', 'idont', 'wondering', 'somethings', 'could', 'loveee', 'create', 'iove', 'mony', 'shouldent', 'need', 'tycons', 'better', 'git', 'fun', 'lods', 'moneyand', 'get', 'thing', 'buy', 'love', 'cuter', 'like', 'roux', 'cast', 'buying', 'good', 'robix', 'robuxs', 'cheeper', 'multilayer', 'kindof', 'freebut', 'moneybut', 'wish', 'cost', 'needs', 'stuff', 'nead', 'friendsmeet', 'gat', 'money', 'stuffbut', 'game', 'great', 'dont']\n",
      "Topic 3 확장 키워드 (47개): ['itsometimes', 'harassment', 'rproleplay', 'gamess', 'idont', 'guyss', 'hurtful', 'somethings', 'troller', 'hashtaged', 'butwhen', 'really', 'tycons', 'shouldent', 'hastags', 'git', 'fun', 'lods', 'cool', 'get', 'thing', 'love', 'like', 'entertaning', 'people', 'hacker', 'good', 'mispell', 'multilayer', 'kindof', 'poeple', 'hash', 'stuff', 'gat', 'inaproprite', 'great', 'others', 'game', 'stuffbut', 'dont', 'band', 'bad', 'becauseits', 'nice', 'roblox', 'exploiter', 'reallly']\n",
      "Topic 4 확장 키워드 (47개): ['gamess', 'guyss', 'woud', 'kid', 'childrens', 'appits', 'create', 'adult', 'use', 'ap', 'whould', 'recomend', 'love', 'recommend', 'reccomend', 'recommened', 'withe', 'teenager', 'iton', 'good', 'think', 'supervise', 'multilayer', 'happier', 'child', 'awesome', 'app', 'amazing', 'used', 'amaze', 'friendsmeet', 'great', 'way', 'game', 'teens', 'age', 'becauseits', 'nice', 'roblox', 'shoud', 'persuade', 'mack', 'id', 'would', 'make', 'superr', 'carter']\n",
      "Topic 5 확장 키워드 (47개): ['lie', 'accessory', 'madcity', 'combo', 'clothing', 'brings', 'arsenal', 'bankrupt', '499', 'plz', 'bring', 'backand', 'colour', 'skin', 'plzzz', 'mor', 'outfits', 'please', 'na', 'premium', 'jailbreak', 'membership', 'removed', 'gon', 'hairs', 'greeting', '450', 'back', 'tixs', 'phantom', 'wan', 'return', 'outfit', 'tone', 'feture', 'pls', 'plss', 'add', 'readd', 'remove', 'hair', 'clothes', 'removal', 'itplease', 'color', 'added', 'insign']\n",
      "Topic 6 확장 키워드 (47개): ['itsometimes', 'loveee', 'gamess', 'iove', 'rproleplay', 'guyss', 'coolest', 'meny', 'create', 'appits', 'muchi', 'really', 'tycons', 'git', 'everi', 'somany', 'fun', 'lods', 'ap', 'friends', 'cool', 'get', 'love', 'like', 'ever', 'entertaning', 'muchits', 'siblings', 'friend', 'everrr', 'multilayer', 'friendsand', 'kindof', 'funnest', 'app', 'play', 'pkay', 'bast', 'muchhh', 'playedi', 'friendsmeet', 'gat', 'game', 'milions', 'becauseits', 'roblox', 'reallly']\n",
      "Topic 7 확장 키워드 (47개): ['rproleplay', 'cant', 'gamess', 'guyss', 'loveee', 'iove', 'fixe', 'gitch', 'tycons', 'somtimes', 'sometimes', 'glitch', 'lods', 'fun', 'workbut', 'barley', 'plz', 'love', 'like', 'tablet', 'phone', 'glitchbug', 'please', 'kindle', 'sometimesbut', 'siblings', 'ipad', 'issue', 'good', 'work', 'multilayer', 'lags', 'fox', 'couldnt', 'fricken', 'worked', 'play', 'sometime', 'pkay', 'fix', 'problemthe', 'lag', 'great', 'pls', 'game', 'problem', 'becauseits']\n",
      "Topic 8 확장 키워드 (47개): ['rat', 'loveee', 'gamess', 'woud', '4', 'star', 'updates', 'iove', 'rproleplay', '3', 'upgraded', 'tycons', '1000000000', '2', '45', 'fun', 'lods', 'whould', 'upgrade', 'love', 'updatei', 'like', 'stari', 'take', '55', 'starsbut', 'updating', 'good', 'loadand', 'multilayer', 'five', 'four', 'great', '5', 'game', 'rate', 'takin', 'gove', 'becauseits', 'nice', 'three', 'update', 'starsand', 'id', 'give', 'would', '5stars']\n",
      "Topic 9 확장 키워드 (47개): ['loveee', 'iove', 'guyss', '4', 'coolest', 'yr', 'afternoon', 'sience', 'ive', 'scince', '3', 'yu', 'since', 'gamesu', 'git', '20172018', '2', 'fun', 'olds', 'year', 'gameive', 'oldi', 'ate', 'get', 'iam', 'love', 'siblings', 'kindof', 'ure', 'funnest', 'play', 'ypur', 'pkay', 'loving', 'bast', 'bin', 'yearsand', 'u', 'playong', 'dayi', 'im', 'gat', 'yrs', 'game', 'playin', '2004', 'roblox']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def remove_emojis_from_tokens(tokens):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "        u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "        u\"\\U00002700-\\U000027BF\"\n",
    "        u\"\\U000024C2-\\U0001F251\"\n",
    "        u\"\\U0001F900-\\U0001F9FF\"\n",
    "        \"]+\", flags=re.UNICODE)\n",
    "    return [t for t in tokens if not emoji_pattern.search(t)]\n",
    "\n",
    "# df_filtered['tokens'] = df_filtered['tokens'].apply(remove_emojis_from_tokens)\n",
    "\n",
    "# Word2Vec 학습\n",
    "model_w2v = Word2Vec(sentences=df_filtered['tokens'],\n",
    "                     vector_size=100, window=5, min_count=5,\n",
    "                     workers=4, sg=1)\n",
    "\n",
    "# 키워드 확장\n",
    "expanded_topic_keywords = []\n",
    "\n",
    "for topic in topic_keywords:\n",
    "    expanded_words = set(topic)\n",
    "    for word in topic:\n",
    "        if word in model_w2v.wv:\n",
    "            similar_words = model_w2v.wv.most_similar(word, topn=3)\n",
    "            for similar_word, score in similar_words:\n",
    "                expanded_words.add(similar_word)\n",
    "    # 최대 47개까지만 유지 -> 가장 작은 값이 47개라\n",
    "    expanded_topic_keywords.append(list(expanded_words)[:47])\n",
    "\n",
    "for i, words in enumerate(expanded_topic_keywords):\n",
    "    print(f\"Topic {i+1} 확장 키워드 ({len(words)}개): {words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f20ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e995323",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "df = df_filtered.copy()\n",
    "df['content'] = df['content'].astype(str).str.lower()\n",
    "\n",
    "# 리뷰별 감성 점수 벡터 생성\n",
    "review_feature_scores = []\n",
    "for text in df['content']:\n",
    "    row_scores = []\n",
    "    for keywords in expanded_topic_keywords:\n",
    "        if any(kw in text for kw in keywords):\n",
    "            score = analyzer.polarity_scores(text)['compound']\n",
    "        else:\n",
    "            score = 0.0\n",
    "        row_scores.append(round(score, 2)) \n",
    "    review_feature_scores.append(row_scores)\n",
    "\n",
    "features_df = pd.DataFrame(review_feature_scores, columns=[f'f{i+1}' for i in range(9)])\n",
    "features_df.insert(0, \"No.\", range(1, len(features_df) + 1))\n",
    "\n",
    "# 평점 이진화\n",
    "df = df.reset_index(drop=True)\n",
    "features_df = features_df.reset_index(drop=True)\n",
    "features_df['Overall Rating'] = df['score'].apply(lambda x: 1 if x >= 3 else 0)\n",
    "\n",
    "print(features_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e2167b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "X = features_df[[f\"f{i+1}\" for i in range(9)]]\n",
    "X = sm.add_constant(X) \n",
    "y = features_df[\"Overall Rating\"]\n",
    "\n",
    "model = sm.Logit(y, X)\n",
    "result = model.fit()\n",
    "\n",
    "odds_ratios = np.exp(result.params)\n",
    "conf_int = result.conf_int()\n",
    "conf_int_exp = np.exp(conf_int)\n",
    "summary_df = pd.DataFrame({\n",
    "    \"Coefficient\": result.params,\n",
    "    \"Odds Ratio\": odds_ratios,\n",
    "    \"CI Lower\": conf_int_exp[0],\n",
    "    \"CI Upper\": conf_int_exp[1],\n",
    "    \"P-value\": result.pvalues\n",
    "})\n",
    "\n",
    "summary_df = summary_df.round(4)\n",
    "summary_df.index.name = \"Variable\"\n",
    "\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d95515e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
